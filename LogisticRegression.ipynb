{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beabafe3",
   "metadata": {},
   "source": [
    "Модель логистической регрессии, реализованная на Python с использованием библиотек Pandas и NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b03f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43dde47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=10000, n_features=14, n_informative=10, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "X.columns = [f'col_{col}' for col in X.columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28220836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogReg():\n",
    "    \"\"\"\n",
    "    Логистическая регрессия\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : int, optional\n",
    "        Количество шагов градиентного спуска, by default 10\n",
    "    learning_rate : float, optional\n",
    "        Коэффициент скорости обучения градиентного спуска, by default 0.1\n",
    "        Если на вход пришла lambda-функция, то learning_rate вычисляется на каждом шаге на основе переданной функцией\n",
    "    weights : np.ndarray, optional\n",
    "        Веса модели\n",
    "    metric : str, optional\n",
    "        Метрика, которая будет вычисляться параллельно с функцией потерь.\n",
    "        Принимает одно из следующих значений: accuracy, precision, recall, f1, roc_auc, by default None\n",
    "    reg : str, optional\n",
    "        Вид регуляризации. Принимает одно из следующих значений: l1, l2, elasticnet, by default None\n",
    "    l1_coef : float, optional\n",
    "        Коэффициент L1 регуляризации. Принимает значения от 0.0 до 1.0, by default 0\n",
    "    l2_coef : float, optional\n",
    "        Коэффициент L2 регуляризации. Принимает значения от 0.0 до 1.0, by default 0\n",
    "    sgd_sample : Union[int, float], optional\n",
    "        Количество образцов, которое будет использоваться на каждой итерации обучения.\n",
    "        Может принимать целые числа, либо дробные от 0.0 до 1.0, by default None\n",
    "    random_state : int, optional\n",
    "        Сид для воспроизводимости результата, by default 42\n",
    "    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def timer(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.perf_counter()\n",
    "            val = func(*args, **kwargs)\n",
    "            end = time.perf_counter()\n",
    "            work_time = end - start\n",
    "            print(f'Время выполнения {func.__name__}: {round(work_time, 4)} сек.')\n",
    "            return val\n",
    "        return wrapper\n",
    "    \n",
    "    @staticmethod\n",
    "    def indicator(numbers: pd.Series) -> pd.Series:\n",
    "        numbers[numbers > 0.5] = 1\n",
    "        numbers[numbers <= 0.5] = 0\n",
    "        return numbers\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "        numerator = MyLogReg.indicator(y_pred)[MyLogReg.indicator(y_true) == MyLogReg.indicator(y_pred)].count()\n",
    "        denominator = y_true.shape[0]\n",
    "        return round(numerator / denominator, 10)\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "        numerator = MyLogReg.indicator(y_true)[(MyLogReg.indicator(y_pred) == 1) & (MyLogReg.indicator(y_true) == 1)].count()\n",
    "        denominator = MyLogReg.indicator(y_pred)[MyLogReg.indicator(y_pred) == 1].count()\n",
    "        return round(numerator / denominator, 10)\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "        numerator = MyLogReg.indicator(y_pred)[(MyLogReg.indicator(y_pred) == 1) & (MyLogReg.indicator(y_true) == 1)].count()\n",
    "        denominator = MyLogReg.indicator(y_pred)[MyLogReg.indicator(y_true) == 1].count()\n",
    "        return round(numerator / denominator, 10)\n",
    "    \n",
    "    @staticmethod\n",
    "    def f1(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "        result = 2 * MyLogReg.precision(y_true, y_pred) * MyLogReg.recall(y_true, y_pred) / (MyLogReg.precision(y_true, y_pred) + MyLogReg.recall(y_true, y_pred))\n",
    "        return round(result, 10)\n",
    "    \n",
    "    @staticmethod\n",
    "    def roc_auc(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "        sqr = 0\n",
    "        n_ones = np.sum(y_true == 1)\n",
    "        n_zeroes = np.sum(y_true == 0)\n",
    "        m = n_ones * n_zeroes\n",
    "        trip = sorted(zip(y_pred, y_true), reverse=True)\n",
    "        for _, true in trip:\n",
    "            if true == 1:\n",
    "                sqr += n_zeroes\n",
    "            else:\n",
    "                n_zeroes -= 1\n",
    "        return sqr / m        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_metric_score(y_true: pd.Series, y_pred: pd.Series, metric: str) -> float:\n",
    "        if metric == 'accuracy':\n",
    "            return MyLogReg.accuracy(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            return MyLogReg.precision(y_true, y_pred)\n",
    "        elif metric == 'recall':\n",
    "            return MyLogReg.recall(y_true, y_pred)\n",
    "        elif metric == 'f1':\n",
    "            return MyLogReg.f1(y_true, y_pred)\n",
    "        elif metric == 'roc_auc':\n",
    "            return MyLogReg.roc_auc(y_true, y_pred)    \n",
    "        \n",
    "    def __calc_grad(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        y_ = (1 + np.exp(- X @ self.weights)) ** (-1)\n",
    "        gradient = (y_ - y) @ X / y.size       \n",
    "        if self.reg:\n",
    "            if self.reg == 'l1' or self.reg == 'elasticnet':\n",
    "                gradient += self.l1_coef * np.sign(self.weights)\n",
    "            if self.reg == 'l2' or self.reg == 'elasticnet':\n",
    "                gradient += self.l2_coef * 2 * self.weights        \n",
    "        return gradient\n",
    "    \n",
    "    def __calc_loss(self, X: pd.DataFrame, y: pd.Series) -> float:\n",
    "        eps = 1e-15    \n",
    "        y_ = (1 + np.exp(- X @ self.weights)) ** (-1)\n",
    "        log_loss = (-(y * np.log(y_ + eps) - (1 - y) * np.log(1 - y_ + eps))).mean()\n",
    "        return log_loss\n",
    "       \n",
    "    def __init__(self, \n",
    "                 n_iter: int = 10, \n",
    "                 learning_rate: float = 0.1, \n",
    "                 weights: np.ndarray = None, \n",
    "                 metric: str = None, \n",
    "                 reg: str = None, \n",
    "                 l1_coef: float = None, \n",
    "                 l2_coef: float = None,\n",
    "                 sgd_sample: float = None,\n",
    "                 random_state: int = 42) -> None:\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    @timer\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose: int = False) -> None:   \n",
    "         \"\"\"Обучение линейной регрессии\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Все фичи\n",
    "        y : pd.Series\n",
    "            Целевая переменная\n",
    "        verbose : int, optional\n",
    "            Указывает через сколько итераций градиентного спуска будет выводиться лог\n",
    "        \"\"\"\n",
    "         random.seed(self.random_state)\n",
    "         X = X.copy()\n",
    "         X.insert(value=1, loc=0, column='w0')\n",
    "         self.weights = np.ones(X.shape[1])      \n",
    "         for i in range(1, self.n_iter + 1):\n",
    "            \n",
    "            #разделение трейна на батчи\n",
    "            if isinstance(self.sgd_sample, float):\n",
    "                sample_rows_idx = random.sample(range(X.shape[0]), round(X.shape[0] * self.sgd_sample))                \n",
    "            if isinstance(self.sgd_sample, int):\n",
    "                sample_rows_idx = random.sample(range(X.shape[0]), self.sgd_sample)            \n",
    "            if not self.sgd_sample:\n",
    "                sample_rows_idx = range(X.shape[0])                                                \n",
    "            X_batch = X.iloc[sample_rows_idx]\n",
    "            y_batch = y.iloc[sample_rows_idx]\n",
    "            \n",
    "            #вычисление лосса и учет регулиризации\n",
    "            log_loss = self.__calc_loss(X_batch, y_batch)            \n",
    "            if self.reg == 'l1' or self.reg == 'elasticnet':\n",
    "                log_loss += self.l1_coef * np.sign(self.weights)\n",
    "            if self.reg == 'l2' or self.reg == 'elasticnet':\n",
    "                log_loss += self.l1_coef * self.weights * 2\n",
    "                \n",
    "            #вычисление весов и скорости градиентного спуска    \n",
    "            if isinstance(self.learning_rate, (int, float)):    \n",
    "                self.weights -= self.__calc_grad(X_batch, y_batch) * self.learning_rate\n",
    "            else:\n",
    "                self.weights -= self.__calc_grad(X_batch, y_batch) * self.learning_rate(i)\n",
    "            \n",
    "            #вычисление последнего значения метрики\n",
    "            if self.metric:\n",
    "                y_ = (1 + np.exp(- X @ self.weights)) ** (-1)\n",
    "                self.score = MyLogReg.get_metric_score(y_, y, self.metric)\n",
    "            \n",
    "            #вывод лога\n",
    "            if verbose and metric:\n",
    "                if (i + 1) % verbose == 0:\n",
    "                    print(f'{i} | loss: {MSE} | {self.metric}: ')\n",
    "                elif i == 0:\n",
    "                    print(f'start | loss: {MSE} | {self.metric}: ')                \n",
    "    \n",
    "    @timer\n",
    "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Выдача предсказаных классов моделью\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Матрица фичей\n",
    "        \"\"\"\n",
    "        X.insert(value=1, loc=0, column='w0')\n",
    "        s = 0\n",
    "        y_ = (1 + np.exp(- X @ self.weights)) ** (-1)\n",
    "        for i in range(y_.shape[0]):\n",
    "            if y_[i] > 0.5:\n",
    "                s += 1\n",
    "        return s\n",
    "    \n",
    "    @timer\n",
    "    def predict_proba(self, X: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Выдача предсказаных вероятностей моделью\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Матрица фичей\n",
    "        \"\"\"\n",
    "        return (1 + np.exp(- X @ self.weights[1:])) ** (-1)\n",
    "                        \n",
    "    def get_coef(self) -> pd.Series:\n",
    "        \"\"\"Возвращает значений весов\"\"\"\n",
    "        \n",
    "        return self.weights[1:]\n",
    "\n",
    "    def get_best_score(self) -> float:\n",
    "        \"\"\"Возвращает последнее значение метрики\"\"\"\n",
    "        \n",
    "        return self.score     \n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f'MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9c8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = MyLogReg(\n",
    "    n_iter = 100, \n",
    "    learning_rate = lambda iter: 0.7 * (0.9 ** iter), \n",
    "    metric = 'roc_auc', \n",
    "    reg = 'elasticnet', \n",
    "    l1_coef = 0.1, \n",
    "    l2_coef = 0.001,\n",
    "    sgd_sample = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c950b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения fit: 0.6558 сек.\n",
      "Время выполнения predict_proba: 0.0006 сек.\n",
      "0.8788968944099379\n"
     ]
    }
   ],
   "source": [
    "LogReg.fit(X_train, y_train)\n",
    "y_pred = LogReg.predict_proba(X_test)\n",
    "print(LogReg.get_best_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee54cd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8164727954971859\n"
     ]
    }
   ],
   "source": [
    "y_pred_sklearn = LogisticRegression().fit(X_train, y_train).predict(X_test)\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
